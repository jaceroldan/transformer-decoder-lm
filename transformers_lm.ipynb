{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhEAZkOgxvht"
      },
      "source": [
        "# Loading the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNQ9AWym0Gku",
        "outputId": "db6c9bbe-3626-4c22-e5f3-fed6a12fb899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1oS2hPvQ0Y3I"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/Coleridge/datasets/train.csv')\n",
        "train_items = train.sample(n=1000, random_state=42)\n",
        "\n",
        "X_train, X_test = train_test_split(train_items, test_size=0.1, random_state=42)\n",
        "train_papers = {}\n",
        "test_papers = {}\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "    curr_path = os.path.join(\n",
        "        os.getcwd(),\n",
        "        'drive',\n",
        "        'My Drive',\n",
        "        'Coleridge',\n",
        "        'datasets',\n",
        "        'train',\n",
        "        X_train.iloc[i]['Id'] + '.json')\n",
        "    with open(curr_path, 'r') as file:\n",
        "        curr_json = json.load(file)\n",
        "        train_papers[X_train.iloc[i]['Id']] = curr_json\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    curr_path = os.path.join(\n",
        "        os.getcwd(),\n",
        "        'drive',\n",
        "        'My Drive',\n",
        "        'Coleridge',\n",
        "        'datasets',\n",
        "        'train',\n",
        "        X_test.iloc[i]['Id'] + '.json')\n",
        "    with open(curr_path, 'r') as file:\n",
        "        curr_json = json.load(file)\n",
        "        test_papers[X_test.iloc[i]['Id']] = curr_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w6pxlbiV2ZXy"
      },
      "outputs": [],
      "source": [
        "# Extract text data from your papers\n",
        "def extract_text(papers):\n",
        "    texts = []\n",
        "    for paper_id, content in papers.items():\n",
        "        # Assuming each paper JSON has a key 'text' or 'content' for text data\n",
        "        paper_text = \" \".join([section['text'] for section in content])  # Adjust if the structure is different\n",
        "        texts.append(paper_text)\n",
        "    return texts\n",
        "\n",
        "# Extract training and testing data\n",
        "train_texts = extract_text(train_papers)\n",
        "test_texts = extract_text(test_papers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB8KKRnr0FhU"
      },
      "source": [
        "# Trigram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9tk-VK-h1f7A"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict, Counter\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KEP5EUv01jjQ"
      },
      "outputs": [],
      "source": [
        "# Step 1: Preprocessing to generate unigrams, bigrams, and trigrams\n",
        "def generate_ngrams(text):\n",
        "    tokens = text.split()\n",
        "    unigrams = tokens\n",
        "    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens) - 1)]\n",
        "    trigrams = [(tokens[i], tokens[i+1], tokens[i+2]) for i in range(len(tokens) - 2)]\n",
        "    return unigrams, bigrams, trigrams\n",
        "\n",
        "# Example corpus\n",
        "corpus = ''.join(train_texts)\n",
        "\n",
        "# Generate unigrams, bigrams, and trigrams from the corpus\n",
        "unigrams, bigrams, trigrams = generate_ngrams(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sVVvs72j2kN_"
      },
      "outputs": [],
      "source": [
        "# Step 2: Build a vocabulary and map words to indices\n",
        "vocab = set(corpus.split())\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nGKpfsVg1pDm"
      },
      "outputs": [],
      "source": [
        "# Step 3: Count frequencies\n",
        "unigram_counts = Counter(unigrams)\n",
        "bigram_counts = Counter(bigrams)\n",
        "trigram_counts = Counter(trigrams)\n",
        "total_unigrams = len(unigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z_Rr9Oje2XaE"
      },
      "outputs": [],
      "source": [
        "# Step 4: Create training data for the model\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for w1, w2, w3 in trigrams:\n",
        "    X_train.append((word_to_idx[w1], word_to_idx[w2]))\n",
        "    y_train.append(word_to_idx[w3])\n",
        "\n",
        "# Convert training data to tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EyrRec-H1qlC"
      },
      "outputs": [],
      "source": [
        "# Step 5: Define the model\n",
        "class TrigramModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(TrigramModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.linear1 = nn.Linear(embed_dim * 2, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = x.view((x.shape[0], -1))  # Flatten\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model and train\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 50  # Embedding size\n",
        "model = TrigramModel(vocab_size, embed_dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Prepare dataset for batch processing\n",
        "dataset = TensorDataset(X_train, y_train)\n",
        "batch_size = 512  # Adjust based on available memory\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "Pv8H4qH6P1l2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OL0Oy_UZ27d9"
      },
      "outputs": [],
      "source": [
        "# Step 6: Interpolation functions\n",
        "def unigram_prob(word):\n",
        "    return unigram_counts[idx_to_word[word]] / total_unigrams\n",
        "\n",
        "def bigram_prob(w2, w1):\n",
        "    return bigram_counts[(idx_to_word[w1], idx_to_word[w2])] / unigram_counts[idx_to_word[w1]] if unigram_counts[idx_to_word[w1]] > 0 else 0\n",
        "\n",
        "def trigram_prob(w3, w1, w2):\n",
        "    return trigram_counts[(idx_to_word[w1], idx_to_word[w2], idx_to_word[w3])] / bigram_counts[(idx_to_word[w1], idx_to_word[w2])] if bigram_counts[(idx_to_word[w1], idx_to_word[w2])] > 0 else 0\n",
        "\n",
        "def interpolated_prob(w3, w1, w2, lambda1=0.1, lambda2=0.3, lambda3=0.6):\n",
        "    p1 = unigram_prob(w3)\n",
        "    p2 = bigram_prob(w3, w2)\n",
        "    p3 = trigram_prob(w3, w1, w2)\n",
        "    return lambda1 * p1 + lambda2 * p2 + lambda3 * p3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Trigram Model"
      ],
      "metadata": {
        "id": "2QNEVUvRpC7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Try to integrate GradScaler\n",
        "import os\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Function to save checkpoint\n",
        "def save_checkpoint(model, optimizer, epoch, loss, checkpoint_dir='checkpoints'):\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch}.pth')\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "# Function to load checkpoint\n",
        "def load_checkpoint(checkpoint_path, model, optimizer=None):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "\n",
        "    print(f\"Checkpoint loaded: Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "    return epoch, loss\n",
        "\n",
        "# Function to calculate interpolated probability\n",
        "def interpolated_prob(w3_idx, w1_idx, w2_idx, lambda1=0.1, lambda2=0.3, lambda3=0.6):\n",
        "    word3 = idx_to_word[w3_idx]\n",
        "    word1 = idx_to_word[w1_idx]\n",
        "    word2 = idx_to_word[w2_idx]\n",
        "\n",
        "    p1 = unigram_prob(w3_idx)  # Unigram probability\n",
        "    p2 = bigram_prob(w3_idx, w2_idx)  # Bigram probability\n",
        "    p3 = trigram_prob(w3_idx, w1_idx, w2_idx)  # Trigram probability\n",
        "\n",
        "    return lambda1 * p1 + lambda2 * p2 + lambda3 * p3\n",
        "\n",
        "# Function to calculate log-likelihood and perplexity using interpolated probabilities\n",
        "def calculate_metrics_with_interpolation(model, data_loader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_log_likelihood = 0.0\n",
        "    total_words = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for inputs, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Calculate log-likelihood using interpolated probabilities\n",
        "            for i in range(len(inputs)):\n",
        "                w1_idx, w2_idx = inputs[i][0].item(), inputs[i][1].item()\n",
        "                w3_idx = targets[i].item()\n",
        "\n",
        "                # Get interpolated probability\n",
        "                prob = interpolated_prob(w3_idx, w1_idx, w2_idx)\n",
        "\n",
        "                # Avoid log(0) by ensuring prob is non-zero\n",
        "                if prob > 0:\n",
        "                    log_prob = torch.log(torch.tensor(prob))\n",
        "                else:\n",
        "                    log_prob = torch.tensor(-float('inf'))  # Log of zero case\n",
        "\n",
        "                total_log_likelihood += log_prob.item()\n",
        "                total_words += 1\n",
        "\n",
        "    # Average log-likelihood\n",
        "    average_log_likelihood = total_log_likelihood / total_words\n",
        "\n",
        "    # Perplexity: exp(-average log-likelihood)\n",
        "    perplexity = torch.exp(-torch.tensor(average_log_likelihood))\n",
        "\n",
        "    return total_log_likelihood, perplexity.item()\n",
        "\n",
        "# Modify the training loop to calculate metrics with interpolation\n",
        "def train_and_evaluate_model_with_interpolation(model, train_loader, test_loader, num_epochs=10, learning_rate=0.001, checkpoint_dir='checkpoints'):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Move model to the GPU if available\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, (inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:    # Print every 100 batches\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}], Loss: {running_loss / 100:.4f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Save checkpoint after every epoch\n",
        "        save_checkpoint(model, optimizer, epoch+1, loss.item(), checkpoint_dir)\n",
        "\n",
        "        # Calculate log-likelihood and perplexity on the test set using interpolated probabilities\n",
        "        log_likelihood, perplexity = calculate_metrics_with_interpolation(model, test_loader)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Log-Likelihood: {log_likelihood:.4f}, Perplexity: {perplexity:.4f}')\n",
        "\n",
        "# Example: Initialize and train model with evaluation using interpolation\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 50  # Embedding size\n",
        "model = TrigramModel(vocab_size, embed_dim)\n",
        "\n",
        "# Assume `train_loader` and `test_loader` are your DataLoaders\n",
        "# Train and evaluate the model with interpolated probabilities for log-likelihood and perplexity\n",
        "train_and_evaluate_model_with_interpolation(model, train_loader, test_loader, num_epochs=10)\n",
        "\n",
        "# You can also run evaluation separately on the test set after training using interpolated probabilities\n",
        "log_likelihood, perplexity = calculate_metrics_with_interpolation(model, test_loader)\n",
        "print(f\"Final Evaluation - Log-Likelihood: {log_likelihood:.4f}, Perplexity: {perplexity:.4f}\")\n"
      ],
      "metadata": {
        "id": "H2mVzTOJrigl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663,
          "referenced_widgets": [
            "f5b3034a672f4967a3ba22ed06768985",
            "874eb6dc0c1642b9a166a86f7db9c317",
            "a8f1a0421fec4021aed92b02db6346a6",
            "a11e68dc1dee46bbbf3abb33151e58b1",
            "49409efdaf14434ab9d575e729691700",
            "976eb8f80022463ea4d1d0e7051ef921",
            "c7ed2b2805304495b1c6b14d8c1b1301",
            "dc782e9950154076a1a9ec904c4bc0a1",
            "f875eb84d8b04235922aa272b750d184",
            "544573ba3b754f05a7f31fa9017b8c7a",
            "57adf55fa0c5422aaa9cbdd509795c84",
            "074c2c132f5a4d63abecf744e90266b2",
            "a469552cd7134d9aae4c62672f295471",
            "c98cf75af5624588ba2cb789bd23cfbc",
            "0a6f4695a7a8414a9b23185c7a20ae41",
            "4e1e24495fe54afa8fe8ed07760e0871",
            "d4dabb572f24403db587eb11c3b240cf",
            "5dd8e1c1f99641639b58f6946baaa572",
            "ca6966e98b034691a9163a09df1a3a74",
            "5c7106f6eb1049e1a8d8d21da277bc98",
            "6da4482c4bec4024ae5d2748c4a5ca47",
            "3b2a8931e44a454799a5917bb7c3b0b2"
          ]
        },
        "id": "hWCP25Eh1tSy",
        "outputId": "37aee742-d786-4b3b-8e0a-11b3ca45e621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-cd088f2a61b2>:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5b3034a672f4967a3ba22ed06768985"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/10:   0%|          | 0/13798 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "074c2c132f5a4d63abecf744e90266b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-cd088f2a61b2>:60: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Batch [100], Loss: 7.8113\n",
            "Epoch [1/10], Batch [200], Loss: 7.8038\n",
            "Epoch [1/10], Batch [300], Loss: 7.7839\n",
            "Epoch [1/10], Batch [400], Loss: 7.7719\n",
            "Epoch [1/10], Batch [500], Loss: 7.7665\n",
            "Epoch [1/10], Batch [600], Loss: 7.7586\n",
            "Epoch [1/10], Batch [700], Loss: 7.7410\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-cd088f2a61b2>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# Train model with GPU and mixed precision training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mtrain_model_with_amp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-cd088f2a61b2>\u001b[0m in \u001b[0;36mtrain_model_with_amp\u001b[0;34m(model, train_loader, num_epochs, learning_rate, checkpoint_dir)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# Scale the loss and backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm.notebook import tqdm  # Use tqdm.notebook for Colab and Jupyter\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Function to save checkpoint\n",
        "def save_checkpoint(model, optimizer, epoch, loss, checkpoint_dir='/content/drive/My Drive/Coleridge/datasets/'):\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f'trigram_model_epoch_{epoch}.pth')\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "# Function to load checkpoint\n",
        "def load_checkpoint(checkpoint_path, model, optimizer=None):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "\n",
        "    print(f\"Checkpoint loaded: Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "    return epoch, loss\n",
        "\n",
        "\n",
        "def train_model_with_amp(model, train_loader, num_epochs=10, learning_rate=0.001, checkpoint_dir='checkpoints'):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Move model to the GPU if available\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, (inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Use autocast for mixed precision training\n",
        "            with autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "            # Scale the loss and backward pass\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:    # Print every 100 batches\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}], Loss: {running_loss / 100:.4f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Save checkpoint after every epoch\n",
        "        save_checkpoint(model, optimizer, epoch+1, loss.item(), checkpoint_dir)\n",
        "\n",
        "\n",
        "# Train model with GPU and mixed precision training\n",
        "train_model_with_amp(model, train_loader, num_epochs=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_corpus = ''.join(test_texts)\n",
        "test_unigrams, test_bigrams, test_trigrams = generate_ngrams(test_corpus)\n",
        "\n",
        "test_vocab = set(test_corpus.split())\n",
        "test_word_to_idx = {word: i for i, word in enumerate(test_vocab)}\n",
        "test_idx_to_word = {i: word for i, word in enumerate(test_vocab)}\n",
        "\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "for w1, w2, w3 in test_trigrams:\n",
        "    X_test.append((test_word_to_idx[w1], test_word_to_idx[w2]))\n",
        "    y_test.append(test_word_to_idx[w3])\n",
        "\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)"
      ],
      "metadata": {
        "id": "BkE-AIt8quCb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of Trigram Model"
      ],
      "metadata": {
        "id": "4QYxiTTzpAos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate log-likelihood and perplexity\n",
        "def calculate_metrics(model, data_loader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_log_likelihood = 0.0\n",
        "    total_words = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for inputs, targets in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate log probabilities (log softmax)\n",
        "            log_probs = F.log_softmax(outputs, dim=1)\n",
        "\n",
        "            # Gather the log likelihoods for the true targets\n",
        "            log_likelihoods = log_probs[range(len(targets)), targets]\n",
        "\n",
        "            # Sum log likelihoods\n",
        "            total_log_likelihood += log_likelihoods.sum().item()\n",
        "            total_words += len(targets)\n",
        "\n",
        "    # Average negative log likelihood\n",
        "    average_log_likelihood = total_log_likelihood / total_words\n",
        "\n",
        "    # Perplexity: exp(-average log-likelihood)\n",
        "    perplexity = torch.exp(-torch.tensor(average_log_likelihood))\n",
        "\n",
        "    return total_log_likelihood, perplexity.item()\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "metrics = calculate_metrics(model, test_loader)"
      ],
      "metadata": {
        "id": "Cpe7w2R0YAp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZwgb0wkxrnH"
      },
      "source": [
        "# Transformer Decoder-Only Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLo1hbHxG9T0"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i24T1NVOIao2"
      },
      "source": [
        "# Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jhv3KdmyH84K"
      },
      "outputs": [],
      "source": [
        "# Extract text data from your papers\n",
        "def extract_text(papers):\n",
        "    texts = []\n",
        "    for paper_id, content in papers.items():\n",
        "        # Assuming each paper JSON has a key 'text' or 'content' for text data\n",
        "        paper_text = \" \".join([section['text'] for section in content])  # Adjust if the structure is different\n",
        "        texts.append(paper_text)\n",
        "    return texts\n",
        "\n",
        "# Extract training and testing data\n",
        "train_texts = extract_text(train_papers)\n",
        "test_texts = extract_text(test_papers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHmV-L25IdFB"
      },
      "source": [
        "# Tokenizing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICP9uGRLIYgl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Initialize tokenizer (or use any other tokenizer that fits your dataset)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize input and output (target) sequences\n",
        "def tokenize_texts(texts):\n",
        "    return tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "# Prepare data (assuming you have train_texts and test_texts)\n",
        "train_input_ids = tokenize_texts(train_texts)['input_ids']\n",
        "train_output_ids = tokenize_texts(test_texts)['input_ids']  # For decoder targets\n",
        "\n",
        "# Use a special token for padding and start/end of sequences if needed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Custom Dataset and DataLoader"
      ],
      "metadata": {
        "id": "dYBx7RyuDYPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, input_ids):\n",
        "        self.input_ids = input_ids  # The tokenized input sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Input sequence (all tokens except the last one)\n",
        "        src = self.input_ids[idx][:-1]\n",
        "        # Target sequence (all tokens except the first one)\n",
        "        tgt = self.input_ids[idx][1:]\n",
        "\n",
        "        return torch.tensor(src), torch.tensor(tgt)\n",
        "\n",
        "\n",
        "# Create the dataset using the tokenized input data (train_input_ids)\n",
        "train_dataset = TextDataset(train_input_ids)\n",
        "\n",
        "# Use DataLoader to handle batching\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=None)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
      ],
      "metadata": {
        "id": "oPZn1E14DXlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7WS42a7UbfM"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_decoder_layers, dim_feedforward, max_len=512, dropout=0.1):\n",
        "        super(DecoderOnlyTransformer, self).__init__()\n",
        "\n",
        "        # Embedding layer for input tokens\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.positional_encoding = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        # Transformer Decoder\n",
        "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        # Output layer to convert decoder output to token logits\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, tgt):\n",
        "        batch_size, seq_len = tgt.size()\n",
        "\n",
        "        # Add position information to the target sequence\n",
        "        positions = torch.arange(0, seq_len).unsqueeze(0).expand(batch_size, seq_len).to(tgt.device)\n",
        "\n",
        "        # Embedding + Positional Encoding\n",
        "        tgt_embedded = self.embedding(tgt) + self.positional_encoding(positions)\n",
        "        tgt_embedded = self.dropout(tgt_embedded)\n",
        "\n",
        "        # Create a square mask of size [seq_len, seq_len]\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(tgt.device)\n",
        "\n",
        "        # Since this is a decoder-only model, pass tgt_embedded as both tgt and memory\n",
        "        decoder_output = self.transformer_decoder(tgt_embedded, memory=tgt_embedded, tgt_mask=tgt_mask)\n",
        "\n",
        "        # Output layer to map decoder output to logits\n",
        "        logits = self.fc_out(decoder_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = tokenizer.vocab_size\n",
        "output_dim = tokenizer.vocab_size\n",
        "d_model = 512\n",
        "nhead = 8\n",
        "num_encoder_layers = 6\n",
        "num_decoder_layers = 6\n",
        "dim_feedforward = 2048\n",
        "\n",
        "model = DecoderOnlyTransformer(tokenizer.vocab_size, d_model, nhead, num_decoder_layers, dim_feedforward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecbZuPKtIzgc"
      },
      "source": [
        "# Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, loss, filename='transformer_checkpoint.pth'):\n",
        "    # Save a dictionary with the model's state, optimizer's state, and other relevant information\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'loss': loss\n",
        "    }\n",
        "    checkpoint_dir = '/content/drive/My Drive/Coleridge/datasets/'\n",
        "    torch.save(checkpoint, checkpoint_dir + filename)\n",
        "    print(f\"Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, filename='transformer_checkpoint.pth'):\n",
        "    checkpoint_dir = '/content/drive/My Drive/Coleridge/datasets/'\n",
        "    if os.path.isfile(checkpoint_dir + filename):\n",
        "        checkpoint = torch.load(checkpoint_dir + filename)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        epoch = checkpoint['epoch']\n",
        "        loss = checkpoint['loss']\n",
        "        print(f\"Checkpoint loaded from epoch {epoch}, loss: {loss:.4f}\")\n",
        "        return epoch, loss\n",
        "    else:\n",
        "        print(\"No checkpoint found at\", checkpoint_dir + filename)\n",
        "        return None, None\n"
      ],
      "metadata": {
        "id": "HQ2uBynTQfeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T10WUUzPI3IS"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm  # Use tqdm.notebook for Colab and Jupyter\n",
        "# Optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "# Training loop with batches\n",
        "num_epochs = 20\n",
        "model.to(device)\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "save_every_n_epochs = 1  # Save a checkpoint every 2 epochs\n",
        "best_loss = float('inf')  # For saving the best model\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Use TQDM to wrap the train_loader for progress bar\n",
        "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
        "        tepoch.set_description(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        for batch in tepoch:\n",
        "            src, tgt = batch\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            print(f\"Shape of src: {src.shape}, Shape of tgt: {tgt.shape}\")\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(src)\n",
        "            loss = criterion(outputs.view(-1, vocab_size), tgt.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Update the progress bar with current loss\n",
        "            tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint every few epochs\n",
        "    if (epoch + 1) % save_every_n_epochs == 0:\n",
        "        save_checkpoint(model, optimizer, epoch + 1, avg_loss, filename=f'checkpoint_epoch_{epoch+1}.pth')\n",
        "\n",
        "    # Save the best model if current epoch loss is better\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        save_checkpoint(model, optimizer, epoch + 1, avg_loss, filename='best_model.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom loading from checkpoint!\n",
        "\n",
        "Uncomment only if we need to read the checkpoints."
      ],
      "metadata": {
        "id": "z6GB4Nm7Qy8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize model and optimizer\n",
        "# model = DecoderOnlyTransformer(vocab_size, d_model, nhead, num_decoder_layers, dim_feedforward)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# # Load checkpoint if available\n",
        "# start_epoch, _ = load_checkpoint(model, optimizer, filename='checkpoint_epoch_2.pth')\n",
        "\n",
        "# if start_epoch is None:\n",
        "#     start_epoch = 0  # Start from scratch if no checkpoint is found\n",
        "\n",
        "# # Continue training from the checkpoint\n",
        "# for epoch in range(start_epoch, num_epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in train_loader:\n",
        "#         src, tgt = batch\n",
        "#         src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         outputs = model(src)\n",
        "#         loss = criterion(outputs.view(-1, vocab_size), tgt.view(-1))\n",
        "\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     avg_loss = total_loss / len(train_loader)\n",
        "#     print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "#     # Save checkpoints as before\n"
      ],
      "metadata": {
        "id": "BE6wNbi1QxsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjx9yPoLK6Wj"
      },
      "source": [
        "# Test Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55aAWTZ7K77t"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_correct = 0\n",
        "    num_total = 0\n",
        "\n",
        "    with torch.no_grad():  # No gradient calculation during evaluation\n",
        "        for batch in dataloader:\n",
        "            src, tgt = batch  # Unpack batch\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            print(f\"Shape of src: {src.shape}, Shape of tgt: {tgt.shape}\")\n",
        "\n",
        "            # Forward pass through the model\n",
        "            outputs = model(src)\n",
        "            loss = criterion(outputs.view(-1, vocab_size), tgt.view(-1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Accuracy calculation\n",
        "            predicted_tokens = outputs.argmax(-1)  # Get the token with the highest score\n",
        "            num_correct += (predicted_tokens == tgt).sum().item()\n",
        "            num_total += tgt.numel()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = num_correct / num_total if num_total > 0 else 0\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Evaluation after training\n",
        "test_dataset = TextDataset(train_output_ids)  # Prepare test dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample training on smaller dataset"
      ],
      "metadata": {
        "id": "63FVz_vwo9eG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXr7cF4Lo9N1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f5b3034a672f4967a3ba22ed06768985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_874eb6dc0c1642b9a166a86f7db9c317",
              "IPY_MODEL_a8f1a0421fec4021aed92b02db6346a6",
              "IPY_MODEL_a11e68dc1dee46bbbf3abb33151e58b1"
            ],
            "layout": "IPY_MODEL_49409efdaf14434ab9d575e729691700"
          }
        },
        "874eb6dc0c1642b9a166a86f7db9c317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_976eb8f80022463ea4d1d0e7051ef921",
            "placeholder": "",
            "style": "IPY_MODEL_c7ed2b2805304495b1c6b14d8c1b1301",
            "value": "Epochs:0%"
          }
        },
        "a8f1a0421fec4021aed92b02db6346a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc782e9950154076a1a9ec904c4bc0a1",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f875eb84d8b04235922aa272b750d184",
            "value": 0
          }
        },
        "a11e68dc1dee46bbbf3abb33151e58b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_544573ba3b754f05a7f31fa9017b8c7a",
            "placeholder": "",
            "style": "IPY_MODEL_57adf55fa0c5422aaa9cbdd509795c84",
            "value": "0/10[00:58&lt;?,?it/s]"
          }
        },
        "49409efdaf14434ab9d575e729691700": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "976eb8f80022463ea4d1d0e7051ef921": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7ed2b2805304495b1c6b14d8c1b1301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc782e9950154076a1a9ec904c4bc0a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f875eb84d8b04235922aa272b750d184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "544573ba3b754f05a7f31fa9017b8c7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57adf55fa0c5422aaa9cbdd509795c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "074c2c132f5a4d63abecf744e90266b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a469552cd7134d9aae4c62672f295471",
              "IPY_MODEL_c98cf75af5624588ba2cb789bd23cfbc",
              "IPY_MODEL_0a6f4695a7a8414a9b23185c7a20ae41"
            ],
            "layout": "IPY_MODEL_4e1e24495fe54afa8fe8ed07760e0871"
          }
        },
        "a469552cd7134d9aae4c62672f295471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4dabb572f24403db587eb11c3b240cf",
            "placeholder": "",
            "style": "IPY_MODEL_5dd8e1c1f99641639b58f6946baaa572",
            "value": "Epoch1/10:6%"
          }
        },
        "c98cf75af5624588ba2cb789bd23cfbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca6966e98b034691a9163a09df1a3a74",
            "max": 13798,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c7106f6eb1049e1a8d8d21da277bc98",
            "value": 763
          }
        },
        "0a6f4695a7a8414a9b23185c7a20ae41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6da4482c4bec4024ae5d2748c4a5ca47",
            "placeholder": "",
            "style": "IPY_MODEL_3b2a8931e44a454799a5917bb7c3b0b2",
            "value": "763/13798[00:58&lt;16:07,13.48it/s]"
          }
        },
        "4e1e24495fe54afa8fe8ed07760e0871": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4dabb572f24403db587eb11c3b240cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dd8e1c1f99641639b58f6946baaa572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca6966e98b034691a9163a09df1a3a74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c7106f6eb1049e1a8d8d21da277bc98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6da4482c4bec4024ae5d2748c4a5ca47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b2a8931e44a454799a5917bb7c3b0b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}