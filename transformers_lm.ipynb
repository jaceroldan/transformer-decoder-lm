{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "90d5e796666a46bdb53db8b1f06ed7a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ab13f68a53c470297f51fc650b3b399",
              "IPY_MODEL_8fd490e3bdd04f60bae7500324cf32de",
              "IPY_MODEL_02133dd3f74a461aa5aa0193fea8f85a"
            ],
            "layout": "IPY_MODEL_f538ad7718b14043bbe72db49bb9b4a1"
          }
        },
        "6ab13f68a53c470297f51fc650b3b399": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d3e69c280474d45827df4b7359ff643",
            "placeholder": "​",
            "style": "IPY_MODEL_ca801338639140c1ac284b1c810a0c68",
            "value": "model.safetensors: 100%"
          }
        },
        "8fd490e3bdd04f60bae7500324cf32de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5fa832aee1b44c8b21f7dc0c660d755",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab1c8a7c796f4699953a8c185522f775",
            "value": 440449768
          }
        },
        "02133dd3f74a461aa5aa0193fea8f85a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e56aa09c04c40f89ab8323b766fca47",
            "placeholder": "​",
            "style": "IPY_MODEL_5f142fe70f8443f18891b06ee4b28cc2",
            "value": " 440M/440M [00:06&lt;00:00, 77.8MB/s]"
          }
        },
        "f538ad7718b14043bbe72db49bb9b4a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d3e69c280474d45827df4b7359ff643": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca801338639140c1ac284b1c810a0c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5fa832aee1b44c8b21f7dc0c660d755": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab1c8a7c796f4699953a8c185522f775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e56aa09c04c40f89ab8323b766fca47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f142fe70f8443f18891b06ee4b28cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the datasets"
      ],
      "metadata": {
        "id": "IhEAZkOgxvht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "KNQ9AWym0Gku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/Datasets/Coleridge/datasets/train.csv')\n",
        "train_items = train.sample(n=1000, random_state=42)\n",
        "\n",
        "X_train, X_test = train_test_split(train_items, test_size=0.1, random_state=42)\n",
        "train_papers = {}\n",
        "test_papers = {}\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "    curr_path = os.path.join(\n",
        "        os.getcwd(),\n",
        "        'drive',\n",
        "        'My Drive',\n",
        "        'Datasets',\n",
        "        'Coleridge',\n",
        "        'datasets',\n",
        "        'train',\n",
        "        X_train.iloc[i]['Id'] + '.json')\n",
        "    with open(curr_path, 'r') as file:\n",
        "        curr_json = json.load(file)\n",
        "        train_papers[X_train.iloc[i]['Id']] = curr_json\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    curr_path = os.path.join(\n",
        "        os.getcwd(),\n",
        "        'drive',\n",
        "        'My Drive',\n",
        "        'Datasets',\n",
        "        'Coleridge',\n",
        "        'datasets',\n",
        "        'train',\n",
        "        X_test.iloc[i]['Id'] + '.json')\n",
        "    with open(curr_path, 'r') as file:\n",
        "        curr_json = json.load(file)\n",
        "        test_papers[X_test.iloc[i]['Id']] = curr_json"
      ],
      "metadata": {
        "id": "1oS2hPvQ0Y3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trigram Language Model"
      ],
      "metadata": {
        "id": "gB8KKRnr0FhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import defaultdict\n",
        "import random"
      ],
      "metadata": {
        "id": "9tk-VK-h1f7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Preprocessing to generate unigrams, bigrams, and trigrams\n",
        "def generate_ngrams(text):\n",
        "    tokens = text.split()\n",
        "    unigrams = tokens\n",
        "    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens) - 1)]\n",
        "    trigrams = [(tokens[i], tokens[i+1], tokens[i+2]) for i in range(len(tokens) - 2)]\n",
        "    return unigrams, bigrams, trigrams\n",
        "\n",
        "# Example corpus\n",
        "corpus = \"I am learning NLP. NLP is fun. I love learning about models in NLP.\"\n",
        "\n",
        "# Generate unigrams, bigrams, and trigrams from the corpus\n",
        "unigrams, bigrams, trigrams = generate_ngrams(corpus)"
      ],
      "metadata": {
        "id": "KEP5EUv01jjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Build a vocabulary and map words to indices\n",
        "vocab = set(corpus.split())\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "sVVvs72j2kN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Count frequencies\n",
        "unigram_counts = Counter(unigrams)\n",
        "bigram_counts = Counter(bigrams)\n",
        "trigram_counts = Counter(trigrams)\n",
        "total_unigrams = len(unigrams)"
      ],
      "metadata": {
        "id": "nGKpfsVg1pDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Count frequencies\n",
        "unigram_counts = Counter(unigrams)\n",
        "bigram_counts = Counter(bigrams)\n",
        "trigram_counts = Counter(trigrams)\n",
        "total_unigrams = len(unigrams)"
      ],
      "metadata": {
        "id": "TzeJKxQo2Jdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Create training data for the model\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for w1, w2, w3 in trigrams:\n",
        "    X_train.append((word_to_idx[w1], word_to_idx[w2]))\n",
        "    y_train.append(word_to_idx[w3])\n",
        "\n",
        "# Convert training data to tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)"
      ],
      "metadata": {
        "id": "Z_Rr9Oje2XaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define the model\n",
        "class TrigramModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(TrigramModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.linear1 = nn.Linear(embed_dim * 2, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embeddings(x)\n",
        "        x = x.view((x.shape[0], -1))  # Flatten\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EyrRec-H1qlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Interpolation functions\n",
        "def unigram_prob(word):\n",
        "    return unigram_counts[idx_to_word[word]] / total_unigrams\n",
        "\n",
        "def bigram_prob(w2, w1):\n",
        "    return bigram_counts[(idx_to_word[w1], idx_to_word[w2])] / unigram_counts[idx_to_word[w1]] if unigram_counts[idx_to_word[w1]] > 0 else 0\n",
        "\n",
        "def trigram_prob(w3, w1, w2):\n",
        "    return trigram_counts[(idx_to_word[w1], idx_to_word[w2], idx_to_word[w3])] / bigram_counts[(idx_to_word[w1], idx_to_word[w2])] if bigram_counts[(idx_to_word[w1], idx_to_word[w2])] > 0 else 0\n",
        "\n",
        "def interpolated_prob(w3, w1, w2, lambda1=0.1, lambda2=0.3, lambda3=0.6):\n",
        "    p1 = unigram_prob(w3)\n",
        "    p2 = bigram_prob(w3, w2)\n",
        "    p3 = trigram_prob(w3, w1, w2)\n",
        "    return lambda1 * p1 + lambda2 * p2 + lambda3 * p3"
      ],
      "metadata": {
        "id": "OL0Oy_UZ27d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Train the model with interpolated probabilities\n",
        "def train_model(model, X_train, y_train, num_epochs=10, learning_rate=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Optionally, calculate and display interpolated probabilities\n",
        "        for i in range(5):  # Show interpolated probabilities for 5 random examples\n",
        "            w1, w2 = X_train[i]\n",
        "            w3 = y_train[i]\n",
        "            interp_prob = interpolated_prob(w3.item(), w1.item(), w2.item())\n",
        "            print(f\"Interpolated probability of {idx_to_word[w3.item()]} given {idx_to_word[w1.item()]} {idx_to_word[w2.item()]}: {interp_prob:.4f}\")\n",
        "\n",
        "# Initialize the model and train\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 50  # Embedding size\n",
        "model = TrigramModel(vocab_size, embed_dim)\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "train_model(model, X_train, y_train, num_epochs=10)"
      ],
      "metadata": {
        "id": "hWCP25Eh1tSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Decoder-Only Model"
      ],
      "metadata": {
        "id": "NZwgb0wkxrnH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "eLo1hbHxG9T0",
        "outputId": "740ee554-ae75-4d3f-f1ff-a5d399525635"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-adcadb8c46f6>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from collections import defaultdict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F1dXnKbJH4gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the data"
      ],
      "metadata": {
        "id": "i24T1NVOIao2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text data from your papers\n",
        "def extract_text(papers):\n",
        "    texts = []\n",
        "    for paper_id, content in papers.items():\n",
        "        # Assuming each paper JSON has a key 'text' or 'content' for text data\n",
        "        paper_text = \" \".join([section['text'] for section in content])  # Adjust if the structure is different\n",
        "        texts.append(paper_text)\n",
        "    return texts\n",
        "\n",
        "# Extract training and testing data\n",
        "train_texts = extract_text(train_papers)\n",
        "test_texts = extract_text(test_papers)"
      ],
      "metadata": {
        "id": "Jhv3KdmyH84K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing the Data"
      ],
      "metadata": {
        "id": "aHmV-L25IdFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Initialize tokenizer (or use any other tokenizer that fits your dataset)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize input and output (target) sequences\n",
        "def tokenize_texts(texts):\n",
        "    return tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "# Prepare data (assuming you have train_texts and test_texts)\n",
        "train_input_ids = tokenize_texts(train_texts)['input_ids']\n",
        "train_output_ids = tokenize_texts(test_texts)['input_ids']  # For decoder targets\n",
        "\n",
        "# Use a special token for padding and start/end of sequences if needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "ICP9uGRLIYgl",
        "outputId": "bcc456d8-4282-4120-cc60-51e0b4f0299a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_texts' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-84cdb6f7c2d8>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Prepare data (assuming you have train_texts and test_texts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtrain_output_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# For decoder targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_texts' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, d_model)\n",
        "        self.pos_encoder = nn.Embedding(512, d_model)  # Positional encoding\n",
        "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n",
        "                                          num_encoder_layers=num_encoder_layers,\n",
        "                                          num_decoder_layers=num_decoder_layers,\n",
        "                                          dim_feedforward=dim_feedforward,\n",
        "                                          dropout=dropout)\n",
        "        self.fc_out = nn.Linear(d_model, output_dim)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.embedding(src)  # Encode the input\n",
        "        tgt = self.embedding(tgt)  # Encode the target\n",
        "\n",
        "        src = self.pos_encoder(src)  # Positional encoding for inputs\n",
        "        tgt = self.pos_encoder(tgt)  # Positional encoding for targets\n",
        "\n",
        "        transformer_output = self.transformer(src, tgt)  # Transformer forward pass\n",
        "\n",
        "        output = self.fc_out(transformer_output)  # Final output layer\n",
        "        return output\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = tokenizer.vocab_size\n",
        "output_dim = tokenizer.vocab_size\n",
        "d_model = 512\n",
        "nhead = 8\n",
        "num_encoder_layers = 6\n",
        "num_decoder_layers = 6\n",
        "dim_feedforward = 2048\n",
        "\n",
        "model = TransformerModel(input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)\n"
      ],
      "metadata": {
        "id": "p7WS42a7UbfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating PyTorch Datasets"
      ],
      "metadata": {
        "id": "sm54grf2IsGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torch.utils.data import Dataset\n",
        "\n",
        "# class TextDataset(Dataset):\n",
        "#     def __init__(self, encodings):\n",
        "#         self.encodings = encodings\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings['input_ids'])\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "#         return item\n",
        "\n",
        "# # Create train and test datasets\n",
        "# train_dataset = TextDataset(train_encodings)\n",
        "# test_dataset = TextDataset(test_encodings)\n"
      ],
      "metadata": {
        "id": "irhL-x2DIujY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the model"
      ],
      "metadata": {
        "id": "0ECD7FsyIwz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import BertForSequenceClassification\n",
        "\n",
        "# # Initialize the model for binary or multi-class classification\n",
        "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Adjust num_labels for your task\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "90d5e796666a46bdb53db8b1f06ed7a2",
            "6ab13f68a53c470297f51fc650b3b399",
            "8fd490e3bdd04f60bae7500324cf32de",
            "02133dd3f74a461aa5aa0193fea8f85a",
            "f538ad7718b14043bbe72db49bb9b4a1",
            "9d3e69c280474d45827df4b7359ff643",
            "ca801338639140c1ac284b1c810a0c68",
            "c5fa832aee1b44c8b21f7dc0c660d755",
            "ab1c8a7c796f4699953a8c185522f775",
            "9e56aa09c04c40f89ab8323b766fca47",
            "5f142fe70f8443f18891b06ee4b28cc2"
          ]
        },
        "id": "ahfbEBN6IyA-",
        "outputId": "30d9ce13-6580-4f70-bc16-cc172155dade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90d5e796666a46bdb53db8b1f06ed7a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model"
      ],
      "metadata": {
        "id": "ecbZuPKtIzgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer and Loss Function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)  # Ignore padding\n",
        "\n",
        "# Training loop\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(len(train_input_ids)):\n",
        "        src = train_input_ids[i].unsqueeze(0).to(device)\n",
        "        tgt = train_output_ids[i].unsqueeze(0).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, tgt[:, :-1])  # Decoder uses everything except the last token as input\n",
        "        loss = criterion(output.view(-1, output_dim), tgt[:, 1:].reshape(-1))  # Shifted targets\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_input_ids)\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save the model after training\n",
        "torch.save(model.state_dict(), 'transformer_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T10WUUzPI3IS",
        "outputId": "2bc0cd05-94bb-40bb-9c5b-973c30397892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss is None!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Eval"
      ],
      "metadata": {
        "id": "rjx9yPoLK6Wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "for batch in test_loader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "        # You can extract logits and compute accuracy or other metrics here\n"
      ],
      "metadata": {
        "id": "55aAWTZ7K77t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}